{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"figs/LogoUFSCar.jpg\" alt=\"Logo UFScar\" width=\"110\" align=\"left\"/>  <br/> <center>Universidade Federal de São Carlos (UFSCar)<br/><font size=\"4\"> Departamento de Computação, campus Sorocaba</center></font>\n",
    "</p>\n",
    "\n",
    "<font size=\"5\"><center><b> PROJETO FINAL DE CURSO </b></center></font>\n",
    "<br/>\n",
    "<font size=\"4\"><center><b>Aplicação de métodos de aprendizado de máquina em\n",
    "problemas de processamento de linguagem natural</b></center></font>\n",
    "<br/>\n",
    "<font size=\"4\"><center><b>Grupo 6: Detecção de fake news em notícias brasileiras</b></center></font>\n",
    "<br/>\n",
    "<font size=\"3\"><center><b>Disciplina: Aprendizado de Máquina</b></center></font>\n",
    "<br/>\n",
    "<br/>\n",
    "<font size=\"3\"><center>Prof. Dr. Tiago A. Almeida</center></font>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np #importa a biblioteca usada para trabalhar com vetores de matrizes\n",
    "import pandas as pd #importa a biblioteca usada para trabalhar com dataframes (dados em formato de tabela) e análise de dados\n",
    "import re #Regexr\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('rslp')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.stem import RSLPStemmer #Copyright (C) 2001-2019 NLTK Project\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "\n",
    "treinamento = []\n",
    "bag_of_words = []\n",
    "words = []\n",
    "\n",
    "#os.chdir(r'C:/Users/Gustavo/Desktop/AM/Projeto_G6/')\n",
    "#txt_dataset = open(\"txt_dataset.csv\", \"w\")\n",
    "#txt_dataset.close()\n",
    "\n",
    "#os.chdir(r'C:/Users/Gustavo/Desktop/AM/Projeto_G6/Fake.br-Corpus/full_texts/true/')\n",
    "\n",
    "#Gerar .csv fake\n",
    "#for file in glob.glob(\"*.txt\"):\n",
    "#    news = open(file, \"r\", encoding = \"unicode_escape\")\n",
    "#    sentence = news.read()\n",
    "#    sentence = sentence.replace('\\n',' ')\n",
    "#    words = RemoveStopWords(Stemming(Tokenize(RemoveSymbols(sentence))))\n",
    "#    full_texts = open(\"../../../txt_dataset.csv\", \"a\") #escreve no .csv localizado em C:\\Users\\Gustavo\\Desktop\\AM\\Projeto_G6\n",
    "#    full_texts.write(\"1\") #true\n",
    "#    full_texts.write(\",\") #delimitador de atributo\n",
    "#    for word in words:\n",
    "#        full_texts.write(word)\n",
    "#        full_texts.write(\",\")\n",
    "#    full_texts.write(\"\\n\") #proxima row\n",
    "#    full_texts.close()\n",
    "    \n",
    "#os.chdir(r'C:/Users/Gustavo/Desktop/AM/Projeto_G6/Fake.br-Corpus/full_texts/fake/')\n",
    "\n",
    "#Gerar .csv fake\n",
    "#for file in glob.glob(\"*.txt\"):\n",
    "#    news = open(file, \"r\", encoding = \"unicode_escape\")\n",
    "#    sentence = news.read()\n",
    "#    sentence = sentence.replace('\\n',' ')\n",
    "#    words = RemoveStopWords(Stemming(Tokenize(RemoveSymbols(sentence))))\n",
    "#    full_texts = open(\"../../../txt_dataset.csv\", \"a\") #escreve no .csv localizado em C:\\Users\\Gustavo\\Desktop\\AM\\Projeto_G6\n",
    "#    full_texts.write(\"0\") #fake\n",
    "#    full_texts.write(\",\") #delimitador de atributo\n",
    "#    for word in words:\n",
    "#        full_texts.write(word)\n",
    "#        full_texts.write(\",\")\n",
    "#    full_texts.write(\"\\n\") #proxima row\n",
    "#    full_texts.close()\n",
    "\n",
    "print('Dados carregados com sucesso!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-b3a0b4821b32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExtractWords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mtreinamento\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-cc173435347c>\u001b[0m in \u001b[0;36mExtractWords\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mExtractWords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "with open('txt_dataset.csv', newline='') as csvfile:\n",
    "    dataset = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    #row_count = sum(1 for row in dataset) #nro total de linhas (amostras)\n",
    "    \n",
    "    for row in dataset:\n",
    "        for line in row:\n",
    "            words = ExtractWords(row)\n",
    "        treinamento.append(words)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BagOfWords(word):\n",
    "    if word not in bag_of_words:\n",
    "        if word.isalpha():\n",
    "            bag_of_words.append(word)\n",
    "            #bag_of_words.append(1) #vezes que uma palavra aparece no texto, inicado como 1\n",
    "    #else: #palavra repetida\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveSymbols(sentence):\n",
    "    sentence = re.sub('[^a-z A-Z]', '', sentence) #mantém apenas letras e espaços\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stemming(sentence):\n",
    "    stemmer = RSLPStemmer()\n",
    "    phrase = []\n",
    "    for word in sentence:\n",
    "        phrase.append(stemmer.stem(word.lower()))\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveStopWords(sentence):\n",
    "    stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "    phrase = []\n",
    "    for word in sentence:\n",
    "        if word not in stopwords:\n",
    "            phrase.append(word)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractWords(sentence):\n",
    "    words = sentence.split()\n",
    "    return words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TokenizeSentences(sentences):\n",
    "    words = []\n",
    "    for sentence in sentences:\n",
    "        w = extract_words(sentence)\n",
    "        words.extend(w)\n",
    "        \n",
    "    words = sorted(list(set(words)))\n",
    "    return words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
